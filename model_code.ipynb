{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.196486Z",
     "start_time": "2025-12-23T01:52:38.190849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.221164Z",
     "start_time": "2025-12-23T01:52:38.219296Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "489e366ebdae1f40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.237761Z",
     "start_time": "2025-12-23T01:52:38.226440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Make those seeds so u don't have random result iykwim\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ],
   "id": "c48640b9581168cb",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepping out that data",
   "id": "bdf677253785508c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set Base Varibles",
   "id": "d269f776145d4cb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.246115Z",
     "start_time": "2025-12-23T01:52:38.243157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE =32\n",
    "NUM_WORKERS =0\n",
    "IMG_SIZE = 224 #Use 256 if ur gonna crop it more laterr\n",
    "BASE_DIR = \"Multi-class Weather Dataset\"\n",
    "CLOUDY_DIR = \"Multi-class Weather Dataset/Cloudy\"\n",
    "RAIN_DIR = \"Multi-class Weather Dataset/Rain\"\n",
    "SHINE_DIR = \"Multi-class Weather Dataset/Shine\"\n",
    "SUNRISE_DIR = \"Multi-class Weather Dataset/Sunrise\"\n"
   ],
   "id": "b9d017520272901b",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.255648Z",
     "start_time": "2025-12-23T01:52:38.250430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Basic\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ],
   "id": "b3b5623dc90b0031",
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Transformations",
   "id": "55e770b29e335a56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.266001Z",
     "start_time": "2025-12-23T01:52:38.261172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_transformers = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ],
   "id": "d6cb6ef8914429a6",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.273732Z",
     "start_time": "2025-12-23T01:52:38.271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_transformers = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ],
   "id": "cbda9ed4514e0453",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.281627Z",
     "start_time": "2025-12-23T01:52:38.277858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### Ok so the problem is:\n",
    "# 1.  I have my files like master folder and then each class so i can use the datasets.ImageFolder thingy which makes having the dataset really ez so i don't have to customly define one\n",
    "# 2. BUT i need to apply different transformations on the different splits of the dataset but with the datasets.Image Set up i can only apply one transformation normally\n",
    "\n",
    "## So to fix this issue  I use No transformations at first and then determine the different sizes of the different splits  and THEN i apply the different transformations to each subset that then pass through the the apply_transforms function class thy that then gets transformed and then gets put into these dataloaders\n",
    "\n",
    " ### Don't use this data loading strategy as a model for your other ones."
   ],
   "id": "354ec57d93ec3fa1",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.292315Z",
     "start_time": "2025-12-23T01:52:38.286091Z"
    }
   },
   "cell_type": "code",
   "source": "FULL_dataset = datasets.ImageFolder(BASE_DIR, transform=None)",
   "id": "c04f29a3cef3d3b6",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.300689Z",
     "start_time": "2025-12-23T01:52:38.297961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_size = int(len(FULL_dataset) * 0.8)\n",
    "val_size = int(len(FULL_dataset) * 0.1)\n",
    "test_size = len(FULL_dataset) - train_size-val_size"
   ],
   "id": "b6e05f753a2791c5",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.310940Z",
     "start_time": "2025-12-23T01:52:38.306070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_subset, val_subset, test_subset = random_split(\n",
    "    FULL_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")"
   ],
   "id": "26187f796fada1e9",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.320302Z",
     "start_time": "2025-12-23T01:52:38.315734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_transformations(subset, transforms_top):\n",
    "    class DatasetwitTransformations(Dataset):\n",
    "        def __init__(self, subset, transforms_top):\n",
    "\n",
    "            super().__init__()\n",
    "\n",
    "            self.subset = subset\n",
    "            self.transforms_top = transforms_top\n",
    "\n",
    "            self.classes = subset.dataset.classes\n",
    "            self.class_to_idx = subset.dataset.class_to_idx\n",
    "        def __getitem__(self, idx):\n",
    "\n",
    "            img, label = self.subset[idx]\n",
    "            if self.transforms_top is not None:\n",
    "                img = self.transforms_top(img)\n",
    "            return img, label\n",
    "        def __len__(self):\n",
    "            return len(self.subset)\n",
    "    return DatasetwitTransformations(subset, transforms_top)"
   ],
   "id": "d34cc9d7a25944a7",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.328300Z",
     "start_time": "2025-12-23T01:52:38.325236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset_transformed = apply_transformations(train_subset, train_transformers)\n",
    "valid_dataset_transformed = apply_transformations(val_subset, val_transformers)\n",
    "test_dataset_transformed = apply_transformations(test_subset, val_transformers)"
   ],
   "id": "2d5462467daff317",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.336644Z",
     "start_time": "2025-12-23T01:52:38.332761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset_transformed, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset_transformed, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset_transformed, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False, pin_memory=True)"
   ],
   "id": "e7519e18a4b578fc",
   "outputs": [],
   "execution_count": 129
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.344889Z",
     "start_time": "2025-12-23T01:52:38.341971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_CLASSES = len(FULL_dataset.classes)\n",
    "CLASS_NAMES = FULL_dataset.classes"
   ],
   "id": "cddf4c92efc6e5d9",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.355131Z",
     "start_time": "2025-12-23T01:52:38.350631Z"
    }
   },
   "cell_type": "code",
   "source": "# I needa add more debugging steps like ts",
   "id": "6fb730700adbe8fa",
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.365484Z",
     "start_time": "2025-12-23T01:52:38.360885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'DATALOADING STEPS RESULTS.....')\n",
    "print(f\"\\nNumber of classes: {NUM_CLASSES}\")\n",
    "print(f\"Class Names: {CLASS_NAMES}\")\n",
    "print(f\"\\nNumber of images in Training dataset: {len(train_dataset_transformed)}\")\n",
    "print(f\"Number of images in Validation dataset:{len(valid_dataset_transformed)}\")\n",
    "print(f\"Number of images in Test dataset:{len(test_dataset_transformed)}\")\n",
    "print(\"\\nData loading strat was sucessful\")"
   ],
   "id": "2a9ecb8f6943535f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATALOADING STEPS RESULTS.....\n",
      "\n",
      "Number of classes: 4\n",
      "Class Names: ['Cloudy', 'Rain', 'Shine', 'Sunrise']\n",
      "\n",
      "Number of images in Training dataset: 900\n",
      "Number of images in Validation dataset:112\n",
      "Number of images in Test dataset:113\n",
      "\n",
      "Data loading strat was sucessful\n"
     ]
    }
   ],
   "execution_count": 132
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DEFINING THE CNN",
   "id": "13bea5b3f0a81e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.384316Z",
     "start_time": "2025-12-23T01:52:38.381593Z"
    }
   },
   "cell_type": "code",
   "source": "### Ok making the conv block so ion have to repeat it (I AM NEVER DOING TS AGAIN. IMA JUST USE TRANSFERLEARNING AFTER THIS PROJECT LOL. WHY DID I THINK I NEEDED TO KNOW ALL THISE (althought i learned a lot from this tho)",
   "id": "eb5e0d62bcdaba04",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.395899Z",
     "start_time": "2025-12-23T01:52:38.388596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernal_size=3, pool=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernal_size, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        if self.pool is not None:\n",
    "            x = self.pool(x)\n",
    "        return x\n",
    "\n"
   ],
   "id": "2c13b167340c645a",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.405047Z",
     "start_time": "2025-12-23T01:52:38.401201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WeatherClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3):\n",
    "        super(WeatherClassifierCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            ConvBlock(input_channels, 64, pool=True),\n",
    "            ConvBlock(64, 128, pool=True),\n",
    "            ConvBlock(128, 256, pool=True),\n",
    "            ConvBlock(256, 512, pool=True),\n",
    "        )\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        #classfier part\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n"
   ],
   "id": "e5d3830158db0ba6",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.420086Z",
     "start_time": "2025-12-23T01:52:38.409692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = WeatherClassifierCNN(num_classes=NUM_CLASSES)\n",
    "model = model.to(DEVICE)"
   ],
   "id": "1488ebfc1046484b",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TRAINING PHASE!!!",
   "id": "155c2cbb9f4176f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Traning setup",
   "id": "14f12accc18c84a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.426123Z",
     "start_time": "2025-12-23T01:52:38.424592Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "470ec026731796fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.432399Z",
     "start_time": "2025-12-23T01:52:38.429385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_EPOCH = 10\n",
    "LEARNING_RATE = 0.001\n",
    "best_val_loss = float('inf')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ],
   "id": "eb9377b28a48ad78",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.440655Z",
     "start_time": "2025-12-23T01:52:38.436708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SAVE_DIR = \"checkpoints\"\n",
    "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)"
   ],
   "id": "e6e5b99deae471be",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.449390Z",
     "start_time": "2025-12-23T01:52:38.445360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "schuduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode='min', factor=0.5, patience=5\n",
    ")"
   ],
   "id": "6d307f06daae8aac",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.455425Z",
     "start_time": "2025-12-23T01:52:38.453837Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "432dd779595c9e72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:38.460854Z",
     "start_time": "2025-12-23T01:52:38.458962Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e532f1c8a375164b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training Loop",
   "id": "2af2b0b7dc8a7e2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:45.753118Z",
     "start_time": "2025-12-23T01:52:38.467767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"TRAINING IS STARTING NOW!!\")\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    print(F\"\\n Epoch({epoch+1}/{NUM_EPOCH})\")\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pas\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # the backward pass and step\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Getting results\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted ==  labels).sum().item()\n",
    "\n",
    "        #progress bar update\n",
    "        progress_bar.set_postfix({'loss': loss.item(), 'acc': 100 * train_correct / train_total})\n",
    "\n",
    "    epoch_train_loss = train_loss / train_total\n",
    "    epoch_train_acc = 100 * train_correct / train_total\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    progress_bar = tqdm(valid_loader, desc=\"Validation\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            #forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            progress_bar.set_postfix({'loss' : loss.item(), 'acc': 100 * val_correct / val_total})\n",
    "\n",
    "        epoch_val_loss = val_loss / val_total\n",
    "        epoh_val_acc = 100 * val_correct / val_total\n",
    "\n",
    "        schuduler.step(epoch_val_loss)\n",
    "\n",
    "\n",
    "        print(f'\\nResults......')\n",
    "        print(f\"\\nTrainloss is: {epoch_train_loss:.4} --While Training Accuracy is: {epoch_train_acc:.4}\")\n",
    "        print(f\"\\n Validation loss is: {epoch_val_loss:.4} ---While Validation accuracy is {epoh_val_acc:.4}\")\n",
    "\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_epoch = epoch+1\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'accuracy': best_val_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, Path(SAVE_DIR) / 'best_model.pth')\n",
    "            print(f\"\\nBest Model was Saved in {SAVE_DIR} with the Validation loss of {best_val_loss:.4}\")\n",
    "        else:\n",
    "            print(\"\\nModel Failed to Improve from {best_val_loss:.4} at Epoch {best_epoch}\")\n"
   ],
   "id": "7c449a5a400170ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING IS STARTING NOW!!\n",
      "\n",
      " Epoch(1/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[140]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     20\u001B[39m loss = criterion(outputs, labels)\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# the backward pass and step\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m optimizer.step()\n\u001B[32m     27\u001B[39m \u001B[38;5;66;03m#Getting results\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 140
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Post Project Reflections",
   "id": "d58c474cb417fa8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:45.757279700Z",
     "start_time": "2025-12-23T01:50:16.651717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ok so i need to learn more about the Dataset and Dataloader classes in pytorch\n",
    "# Next project probably litterly the same exact thing except for i just have a different dataset with a different file structure and a use the custom dataset class. I needa rewatch some yt vids I think i might just do a course for now on. AND I JUST DISCOVERED MY GPU ISN'T HAVE BAD TO CODE WITH LOLOLOL its taking about  2 minutes per epoches so i might as well just change it to five epochs total and\n",
    "#\n",
    "#\n",
    "# i don't even know what to type here so i ain't gonna type NOTHING lool\n",
    "#nahh gang ts crazy damm ts crazy ick"
   ],
   "id": "ffcb78c24f624c57",
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "RESULTS AND DATA ANAL",
   "id": "9d60c8250856fcd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:52:45.764709100Z",
     "start_time": "2025-12-23T01:50:16.660816Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2e861c0e4c24be68",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check points!!!!!",
   "id": "2ec2255d10387b3c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-23T01:55:38.558886Z",
     "start_time": "2025-12-23T01:55:38.535652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint =  torch.load(Path(SAVE_DIR) / 'best_model.pth', map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f'\\n MODEL IS LOADED ---Epoch: {checkpoint['epoch']}, Accuracy : {checkpoint['accuracy']:.4}')\n",
    "print(\" Model is LOADED!!!!! Have fun with validation ]\")"
   ],
   "id": "394006f8d5c37f32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MODEL IS LOADED ---Epoch: 5, Accuracy : 0.3411\n",
      " Model is LOADED!!!!! Have fun with validation ]\n"
     ]
    }
   ],
   "execution_count": 143
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Ok we need to make a gradio version of this app",
   "id": "8499630bb4ab810"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eea15a28041028a8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
