{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.484335Z",
     "start_time": "2025-12-22T20:03:32.481152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sched import scheduler\n",
    "\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, Subset\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from pathlib import Path"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 363
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.493267Z",
     "start_time": "2025-12-22T20:03:32.491262Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "489e366ebdae1f40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.509417Z",
     "start_time": "2025-12-22T20:03:32.498370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Make those seeds so u don't have random result iykwim\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n"
   ],
   "id": "c48640b9581168cb",
   "outputs": [],
   "execution_count": 364
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepping out that data",
   "id": "bdf677253785508c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set Base Varibles",
   "id": "d269f776145d4cb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.516746Z",
     "start_time": "2025-12-22T20:03:32.513888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BATCH_SIZE =32\n",
    "NUM_WORKERS =0\n",
    "IMG_SIZE = 224 #Use 256 if ur gonna crop it more laterr\n",
    "BASE_DIR = \"Multi-class Weather Dataset\"\n",
    "CLOUDY_DIR = \"Multi-class Weather Dataset/Cloudy\"\n",
    "RAIN_DIR = \"Multi-class Weather Dataset/Rain\"\n",
    "SHINE_DIR = \"Multi-class Weather Dataset/Shine\"\n",
    "SUNRISE_DIR = \"Multi-class Weather Dataset/Sunrise\"\n"
   ],
   "id": "b9d017520272901b",
   "outputs": [],
   "execution_count": 365
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.526358Z",
     "start_time": "2025-12-22T20:03:32.520091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Basic\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ],
   "id": "b3b5623dc90b0031",
   "outputs": [],
   "execution_count": 366
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Transformations",
   "id": "55e770b29e335a56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.535086Z",
     "start_time": "2025-12-22T20:03:32.531392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_transformers = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.RandomVerticalFlip(0.5),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ],
   "id": "d6cb6ef8914429a6",
   "outputs": [],
   "execution_count": 367
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.542834Z",
     "start_time": "2025-12-22T20:03:32.539428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_transformers = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ],
   "id": "cbda9ed4514e0453",
   "outputs": [],
   "execution_count": 368
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.550814Z",
     "start_time": "2025-12-22T20:03:32.547417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#### Ok so the problem is:\n",
    "# 1.  I have my files like master folder and then each class so i can use the datasets.ImageFolder thingy which makes having the dataset really ez so i don't have to customly define one\n",
    "# 2. BUT i need to apply different transformations on the different splits of the dataset but with the datasets.Image Set up i can only apply one transformation normally\n",
    "\n",
    "## So to fix this issue  I use No transformations at first and then determine the different sizes of the different splits  and THEN i apply the different transformations to each subset that then pass through the the apply_transforms function class thy that then gets transformed and then gets put into these dataloaders\n",
    "\n",
    " ### Don't use this data loading strategy as a model for your other ones."
   ],
   "id": "354ec57d93ec3fa1",
   "outputs": [],
   "execution_count": 369
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.563157Z",
     "start_time": "2025-12-22T20:03:32.555184Z"
    }
   },
   "cell_type": "code",
   "source": "FULL_dataset = datasets.ImageFolder(BASE_DIR, transform=None)",
   "id": "c04f29a3cef3d3b6",
   "outputs": [],
   "execution_count": 370
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.570920Z",
     "start_time": "2025-12-22T20:03:32.567970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_size = int(len(FULL_dataset) * 0.8)\n",
    "val_size = int(len(FULL_dataset) * 0.1)\n",
    "test_size = len(FULL_dataset) - train_size-val_size"
   ],
   "id": "b6e05f753a2791c5",
   "outputs": [],
   "execution_count": 371
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.581298Z",
     "start_time": "2025-12-22T20:03:32.576382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_subset, val_subset, test_subset = random_split(\n",
    "    FULL_dataset, [train_size, val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")"
   ],
   "id": "26187f796fada1e9",
   "outputs": [],
   "execution_count": 372
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.594096Z",
     "start_time": "2025-12-22T20:03:32.590344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_transformations(subset, transforms):\n",
    "    class DatasetwitTransformations(Subset):\n",
    "        def __init__(self, subset, transforms):\n",
    "            super().__init__(subset.dataset, subset.indices)\n",
    "            self.transforms = transforms\n",
    "            self.classes = subset.dataset.classes\n",
    "            self.class_to_idx = subset.dataset.class_to_idx\n",
    "        def __getitem__(self, idx):\n",
    "            the_indices = self.indices[idx]\n",
    "            img, label = self.dataset[the_indices]\n",
    "            if self.transforms:\n",
    "                img = self.transforms(img)\n",
    "            return img, label\n",
    "    return DatasetwitTransformations(subset, transforms)"
   ],
   "id": "d34cc9d7a25944a7",
   "outputs": [],
   "execution_count": 373
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.604094Z",
     "start_time": "2025-12-22T20:03:32.600708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset_transformed = apply_transformations(train_subset, train_transformers)\n",
    "valid_dataset_transformed = apply_transformations(val_subset, val_transformers)\n",
    "test_dataset_transformed = apply_transformations(test_subset, val_transformers)"
   ],
   "id": "2d5462467daff317",
   "outputs": [],
   "execution_count": 374
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.614904Z",
     "start_time": "2025-12-22T20:03:32.610816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset_transformed, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset_transformed, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset_transformed, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False, pin_memory=True)"
   ],
   "id": "e7519e18a4b578fc",
   "outputs": [],
   "execution_count": 375
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.625712Z",
     "start_time": "2025-12-22T20:03:32.621956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_CLASSES = len(FULL_dataset.classes)\n",
    "CLASS_NAMES = FULL_dataset.classes"
   ],
   "id": "cddf4c92efc6e5d9",
   "outputs": [],
   "execution_count": 376
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.636728Z",
     "start_time": "2025-12-22T20:03:32.633953Z"
    }
   },
   "cell_type": "code",
   "source": "# I needa add more debugging steps like ts",
   "id": "6fb730700adbe8fa",
   "outputs": [],
   "execution_count": 377
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.648227Z",
     "start_time": "2025-12-22T20:03:32.643759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'DATALOADING STEPS RESULTS.....')\n",
    "print(f\"\\nNumber of classes: {NUM_CLASSES}\")\n",
    "print(f\"Class Names: {CLASS_NAMES}\")\n",
    "print(f\"\\nNumber of images in Training dataset: {len(train_dataset_transformed)}\")\n",
    "print(f\"Number of images in Validation dataset:{len(valid_dataset_transformed)}\")\n",
    "print(f\"Number of images in Test dataset:{len(test_dataset_transformed)}\")\n",
    "print(\"\\nData loading strat was sucessful\")"
   ],
   "id": "2a9ecb8f6943535f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATALOADING STEPS RESULTS.....\n",
      "\n",
      "Number of classes: 4\n",
      "Class Names: ['Cloudy', 'Rain', 'Shine', 'Sunrise']\n",
      "\n",
      "Number of images in Training dataset: 900\n",
      "Number of images in Validation dataset:112\n",
      "Number of images in Test dataset:113\n",
      "\n",
      "Data loading strat was sucessful\n"
     ]
    }
   ],
   "execution_count": 378
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DEFINING THE CNN",
   "id": "13bea5b3f0a81e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.660892Z",
     "start_time": "2025-12-22T20:03:32.657364Z"
    }
   },
   "cell_type": "code",
   "source": "### Ok making the conv block so ion have to repeat it (I AM NEVER DOING TS AGAIN. IMA JUST USE TRANSFERLEARNING AFTER THIS PROJECT LOL. WHY DID I THINK I NEEDED TO KNOW ALL THISE (althought i learned a lot from this tho)",
   "id": "eb5e0d62bcdaba04",
   "outputs": [],
   "execution_count": 379
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.673185Z",
     "start_time": "2025-12-22T20:03:32.668848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernal_size=3, pool=True):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernal_size, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def __forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        if self.pool is not None:\n",
    "            x = self.pool(x)\n",
    "        return x\n",
    "\n"
   ],
   "id": "2c13b167340c645a",
   "outputs": [],
   "execution_count": 380
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.686282Z",
     "start_time": "2025-12-22T20:03:32.681423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class WeatherClassifierCNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=3):\n",
    "        super(WeatherClassifierCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            ConvBlock(input_channels, 64, pool=True),\n",
    "            ConvBlock(64, 128, pool=True),\n",
    "            ConvBlock(128, 256, pool=True),\n",
    "            ConvBlock(256, 512, pool=True),\n",
    "        )\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        #classfier part\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n"
   ],
   "id": "e5d3830158db0ba6",
   "outputs": [],
   "execution_count": 381
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.710758Z",
     "start_time": "2025-12-22T20:03:32.698284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = WeatherClassifierCNN(num_classes=NUM_CLASSES)\n",
    "model = model.to(DEVICE)"
   ],
   "id": "1488ebfc1046484b",
   "outputs": [],
   "execution_count": 382
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TRAINING PHASE!!!",
   "id": "155c2cbb9f4176f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Traning setup",
   "id": "14f12accc18c84a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.718335Z",
     "start_time": "2025-12-22T20:03:32.716383Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "470ec026731796fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.726448Z",
     "start_time": "2025-12-22T20:03:32.722841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_EPOCH = 30\n",
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.CrossEntropyLoss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ],
   "id": "eb9377b28a48ad78",
   "outputs": [],
   "execution_count": 383
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.735947Z",
     "start_time": "2025-12-22T20:03:32.730969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SAVE_DIR = \"checkpoints\"\n",
    "Path(SAVE_DIR).mkdir(parents=True, exist_ok=True)"
   ],
   "id": "e6e5b99deae471be",
   "outputs": [],
   "execution_count": 384
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.744621Z",
     "start_time": "2025-12-22T20:03:32.741022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "schuduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode='min', factor=0.5, patience=5\n",
    ")"
   ],
   "id": "6d307f06daae8aac",
   "outputs": [],
   "execution_count": 385
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.750986Z",
     "start_time": "2025-12-22T20:03:32.749148Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "432dd779595c9e72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.757824Z",
     "start_time": "2025-12-22T20:03:32.755930Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e532f1c8a375164b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training Loop",
   "id": "2af2b0b7dc8a7e2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.956574Z",
     "start_time": "2025-12-22T20:03:32.765067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"TRAINING IS STARTING NOW!!\")\n",
    "\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    print(F\"\\n Epoch({epoch+1}/{NUM_EPOCH})\")\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pas\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # the backward pass and step\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #Getting results\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted ==  labels.sum.item())\n",
    "\n",
    "        #progress bar update\n",
    "        progress_bar.set_postfix({'loss': loss.item(), 'acc': 100 * train_correct / train_total})\n",
    "\n",
    "    epoch_train_loss = train_loss / train_total\n",
    "    epoch_train_acc = 100 * train_correct / train_total\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    progress_bar = tqdm(valid_loader, desc=\"Validation\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            #forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels.sum().item())\n",
    "\n",
    "            progress_bar.set_postfix({'loss' : loss.item(), 'acc': 100 * val_correct / val_total})\n",
    "\n",
    "        epoch_val_loss = val_loss / val_total\n",
    "        epoh_val_acc = 100 * val_correct / val_total\n",
    "\n",
    "        scheduler.step(epoch_val_loss)\n",
    "\n",
    "\n",
    "        print(f'\\nResults......')\n",
    "        print(f\"\\nTrainloss is: {epoch_train_loss:.4} --While Training Accuracy is: {epoch_train_acc:.4}\")\n",
    "        print(f\"\\n Validation loss is: {epoch_val_loss:.4} ---While Validation accuracy is {epoh_val_acc:.4}\")\n",
    "\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_epoch = epoch+1\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'accuracy': best_val_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, Path(SAVE_DIR) / 'best_model.pth')\n",
    "            print(f\"\\nBest Model was Saved in {SAVE_DIR} with the Validation loss of {best_val_loss:.4}\")\n",
    "        else:\n",
    "            print(\"\\nModel Failed to Improve from {best_val_loss:.4} at Epoch {best_epoch}\")\n",
    "\n",
    "\n"
   ],
   "id": "7c449a5a400170ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING IS STARTING NOW!!\n",
      "\n",
      " Epoch(1/30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[386]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     10\u001B[39m best_val_loss = \u001B[38;5;28mfloat\u001B[39m(\u001B[33m'\u001B[39m\u001B[33minf\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     12\u001B[39m progress_bar = tqdm(train_loader, desc=\u001B[33m\"\u001B[39m\u001B[33mTraining\u001B[39m\u001B[33m\"\u001B[39m, leave=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    729\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    730\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    731\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m732\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    733\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    734\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    735\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    736\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    737\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    738\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    786\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    787\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m788\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    789\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    790\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n\u001B[32m---> \u001B[39m\u001B[32m55\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001B[39m, in \u001B[36mdefault_collate\u001B[39m\u001B[34m(batch)\u001B[39m\n\u001B[32m    337\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdefault_collate\u001B[39m(batch):\n\u001B[32m    338\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    339\u001B[39m \u001B[33;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001B[39;00m\n\u001B[32m    340\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    396\u001B[39m \u001B[33;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[32m    397\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m398\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001B[39m, in \u001B[36mcollate\u001B[39m\u001B[34m(batch, collate_fn_map)\u001B[39m\n\u001B[32m    208\u001B[39m transposed = \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(*batch))  \u001B[38;5;66;03m# It may be accessed twice, so we use a list.\u001B[39;00m\n\u001B[32m    210\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    211\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[32m--> \u001B[39m\u001B[32m212\u001B[39m         \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msamples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    213\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed\n\u001B[32m    214\u001B[39m     ]  \u001B[38;5;66;03m# Backwards compatibility.\u001B[39;00m\n\u001B[32m    215\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    216\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:240\u001B[39m, in \u001B[36mcollate\u001B[39m\u001B[34m(batch, collate_fn_map)\u001B[39m\n\u001B[32m    232\u001B[39m         \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[32m    233\u001B[39m             \u001B[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001B[39;00m\n\u001B[32m    234\u001B[39m             \u001B[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001B[39;00m\n\u001B[32m    235\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[32m    236\u001B[39m                 collate(samples, collate_fn_map=collate_fn_map)\n\u001B[32m    237\u001B[39m                 \u001B[38;5;28;01mfor\u001B[39;00m samples \u001B[38;5;129;01min\u001B[39;00m transposed\n\u001B[32m    238\u001B[39m             ]\n\u001B[32m--> \u001B[39m\u001B[32m240\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(default_collate_err_msg_format.format(elem_type))\n",
      "\u001B[31mTypeError\u001B[39m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>"
     ]
    }
   ],
   "execution_count": 386
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Post Project Reflections",
   "id": "d58c474cb417fa8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T20:03:32.962783300Z",
     "start_time": "2025-12-22T07:08:51.979776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ok so i need to learn more about the Dataset and Dataloader classes in pytorch\n",
    "# Next project probably litterly the same exact thing except for i just have a different dataset with a different file structure and a use the custom dataset class. I needa rewatch some yt vids"
   ],
   "id": "ffcb78c24f624c57",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2e861c0e4c24be68"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
